{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ece7efe",
   "metadata": {},
   "source": [
    "# GAN Project - Creating Monet Style Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37ab4c",
   "metadata": {},
   "source": [
    "https://github.com/youanahabib117/MonetGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc4cfb",
   "metadata": {},
   "source": [
    "### Problem and Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43f262",
   "metadata": {},
   "source": [
    "The aim of this project is to create a GAN function to transform images into a Monet style painting. The dataset contains two sets of images: 300 Monet paintings and 7038 real world photographs, all of size 256x256 pixels. We want to train a model that learns to transform photos into the Monet style and apply that to the set of photos provided. \n",
    "\n",
    "This dataset contains a bit over 7000 photos and is a bit over 400MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd647f",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1404a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb297cc3",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c355e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"gan-getting-started.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"gan_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d791e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_jpg_folder = \"gan_images/monet_jpg\"\n",
    "print(\"Sample file:\", os.listdir(monet_jpg_folder)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64fd870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "photo_jpg_folder = \"gan_images/photo_jpg\"\n",
    "print(\"Sample file:\", os.listdir(photo_jpg_folder)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e33c70",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f311f2",
   "metadata": {},
   "source": [
    "#### Number of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Monet JPG Images:\", len(os.listdir(monet_jpg_folder)))\n",
    "print(\"Total Photo JPG Images:\", len(os.listdir(photo_jpg_folder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2f9f0",
   "metadata": {},
   "source": [
    "We have 300 Monet paintings and 7038 real world photos in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df63d9",
   "metadata": {},
   "source": [
    "#### Loading Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77d507",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_path = os.path.join(monet_jpg_folder, os.listdir(monet_jpg_folder)[0])\n",
    "img = Image.open(img_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1395b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(photo_jpg_folder, os.listdir(photo_jpg_folder)[0])\n",
    "img = Image.open(img_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22f7ad",
   "metadata": {},
   "source": [
    "#### Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76314a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_shapes(path):\n",
    "    shapes = []\n",
    "    for f in os.listdir(path):\n",
    "        img = Image.open(os.path.join(path, f))\n",
    "        shapes.append(img.size)\n",
    "    return Counter(shapes)\n",
    "\n",
    "print(\"Monet shapes:\", get_image_shapes('gan_images/monet_jpg'))\n",
    "print(\"Photo shapes:\", get_image_shapes('gan_images/photo_jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd346863",
   "metadata": {},
   "source": [
    "All of the photos are of size 256x256 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731cd1d0",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38eb01",
   "metadata": {},
   "source": [
    "Rescaling the images to the 0 to 1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61280195",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_dataset = keras.utils.image_dataset_from_directory(\n",
    "    \"gan_images/monet_jpg\",\n",
    "    label_mode=None,\n",
    "    image_size=(256, 256),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "monet_dataset = monet_dataset.map(lambda x: x / 255.0)\n",
    "\n",
    "\n",
    "photo_dataset = keras.utils.image_dataset_from_directory(\n",
    "    \"gan_images/photo_jpg\",\n",
    "    label_mode=None,\n",
    "    image_size=(256, 256),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "photo_dataset = photo_dataset.map(lambda x: x / 255.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6629d",
   "metadata": {},
   "source": [
    "### Data Modeling/Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104031c",
   "metadata": {},
   "source": [
    "This model was taken from https://keras.io/examples/generative/cyclegan/ and adapted to fit this dataset. The model contains a few parts to it. \n",
    "\n",
    "This model has a generator that translates photos into the Monet style and a discriminator that distinguishes between real and fake photos. The model is trained to generate photos in the Monet style that look like the images provided. Essentially, we input RGB images of size 256x256 pixels that are normalized to [-1, 1] and the model outputs images of the same shape and format but with a Monet painting style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38775887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from keras import ops\n",
    "\n",
    "input_img_size = (256, 256, 3)\n",
    "batch_size = 1\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# === Preprocessing ===\n",
    "def normalize_img(img):\n",
    "    img = ops.cast(img, dtype=tf.float32)\n",
    "    return (img / 127.5) - 1.0\n",
    "\n",
    "def preprocess_image(file_path, train=True):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    if train:\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.resize(img, input_img_size[:2])\n",
    "    img = normalize_img(img)\n",
    "    return img\n",
    "\n",
    "# === Dataset Loader ===\n",
    "def get_dataset_from_dir(path, train=True):\n",
    "    files = tf.data.Dataset.list_files(os.path.join(path, \"*.jpg\"))\n",
    "    dataset = files.map(lambda x: preprocess_image(x, train), num_parallel_calls=AUTOTUNE)\n",
    "    return dataset.shuffle(1000).batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "# === Load Datasets ===\n",
    "photo_dataset = get_dataset_from_dir(\"gan_images/photo_jpg\", train=True)   # generator input\n",
    "monet_dataset = get_dataset_from_dir(\"gan_images/monet_jpg\", train=True)   # real images for discriminator\n",
    "\n",
    "# === Zip for GAN training (not paired!)\n",
    "train_dataset = tf.data.Dataset.zip((photo_dataset, monet_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 256\n",
    "batch_size = 1\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_photos = (\n",
    "    tf.data.Dataset.list_files(\"gan_images/photo_jpg/*.jpg\")\n",
    "    .map(preprocess_train_image, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "train_monets = (\n",
    "    tf.data.Dataset.list_files(\"gan_images/monet_jpg/*.jpg\")\n",
    "    .map(preprocess_train_image, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd01da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Initializers\n",
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "# Reflection Padding Layer\n",
    "class ReflectionPadding2D(layers.Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.padding = padding\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        pad_w, pad_h = self.padding\n",
    "        return tf.pad(\n",
    "            input_tensor,\n",
    "            paddings=[[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]],\n",
    "            mode=\"REFLECT\"\n",
    "        )\n",
    "\n",
    "# Residual Block\n",
    "def residual_block(\n",
    "    x,\n",
    "    activation=None,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"valid\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    if activation is None:\n",
    "        activation = layers.Activation(\"relu\")\n",
    "\n",
    "    dim = x.shape[-1]\n",
    "    input_tensor = x\n",
    "\n",
    "    x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n",
    "    x = activation(x)\n",
    "\n",
    "    x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n",
    "\n",
    "    return layers.add([input_tensor, x])\n",
    "\n",
    "# Downsampling Layer\n",
    "def downsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation=None,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    if activation is None:\n",
    "        activation = layers.Activation(\"relu\")\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n",
    "    return activation(x)\n",
    "\n",
    "# Upsampling Layer\n",
    "def upsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation=None,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    if activation is None:\n",
    "        activation = layers.Activation(\"relu\")\n",
    "\n",
    "    x = layers.Conv2DTranspose(\n",
    "        filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n",
    "    return activation(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfaae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape=(256, 256, 3), name=\"generator\"):\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "\n",
    "    # Initial padding + conv\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(inputs)\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        kernel_size=(7, 7),\n",
    "        padding=\"valid\",\n",
    "        kernel_initializer=kernel_init,\n",
    "        use_bias=False,\n",
    "    )(x)\n",
    "    x = layers.GroupNormalization(groups=1, gamma_initializer=gamma_init)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    x = downsample(x, 128)\n",
    "    x = downsample(x, 256)\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(9):\n",
    "        x = residual_block(x)\n",
    "\n",
    "    # Upsampling\n",
    "    x = upsample(x, 128)\n",
    "    x = upsample(x, 64)\n",
    "\n",
    "    # Final conv\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = layers.Conv2D(3, kernel_size=(7, 7), padding=\"valid\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    return keras.Model(inputs, x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_generator(\n",
    "    filters=64,\n",
    "    num_downsampling_blocks=2,\n",
    "    num_residual_blocks=9,\n",
    "    num_upsample_blocks=2,\n",
    "    input_shape=(256, 256, 3),\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    name=\"generator\",\n",
    "):\n",
    "    \"\"\"Creates a ResNet-based generator for Photo → Monet style transfer.\"\"\"\n",
    "\n",
    "    img_input = layers.Input(shape=input_shape, name=f\"{name}_img_input\")\n",
    "\n",
    "    # Initial reflection padding + conv\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size=(7, 7),\n",
    "        padding=\"valid\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    for _ in range(num_downsampling_blocks):\n",
    "        filters *= 2\n",
    "        x = downsample(\n",
    "            x,\n",
    "            filters=filters,\n",
    "            activation=layers.Activation(\"relu\"),\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            gamma_initializer=gamma_initializer,\n",
    "        )\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(\n",
    "            x,\n",
    "            activation=layers.Activation(\"relu\"),\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            gamma_initializer=gamma_initializer,\n",
    "        )\n",
    "\n",
    "    # Upsampling\n",
    "    for _ in range(num_upsample_blocks):\n",
    "        filters //= 2\n",
    "        x = upsample(\n",
    "            x,\n",
    "            filters=filters,\n",
    "            activation=layers.Activation(\"relu\"),\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            gamma_initializer=gamma_initializer,\n",
    "        )\n",
    "\n",
    "    # Final reflection padding + output conv\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = layers.Conv2D(3, kernel_size=(7, 7), padding=\"valid\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    return keras.models.Model(inputs=img_input, outputs=x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aceb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(\n",
    "    filters=64,\n",
    "    input_shape=(256, 256, 3),\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    num_downsampling=3,\n",
    "    name=\"discriminator\"\n",
    "):\n",
    "    \"\"\"Creates a PatchGAN discriminator model for Monet-style detection.\"\"\"\n",
    "\n",
    "    img_input = layers.Input(shape=input_shape, name=f\"{name}_img_input\")\n",
    "\n",
    "    # Initial conv (no normalization on first layer)\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(img_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Downsampling blocks\n",
    "    num_filters = filters\n",
    "    for i in range(num_downsampling):\n",
    "        num_filters *= 2\n",
    "        stride = (2, 2) if i < 2 else (1, 1)  # last block has stride 1\n",
    "\n",
    "        x = layers.Conv2D(\n",
    "            num_filters,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=stride,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            use_bias=False\n",
    "        )(x)\n",
    "        x = layers.GroupNormalization(groups=1, gamma_initializer=gamma_initializer)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Final output layer → PatchGAN output\n",
    "    x = layers.Conv2D(\n",
    "        1,\n",
    "        kernel_size=(4, 4),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "\n",
    "    return keras.models.Model(inputs=img_input, outputs=x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46059b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"Callback to generate and save translated Monet images from test photos.\"\"\"\n",
    "\n",
    "    def __init__(self, test_dataset, num_img=4, output_dir=\"generated\"):\n",
    "        super().__init__()\n",
    "        self.test_dataset = test_dataset\n",
    "        self.num_img = num_img\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        _, ax = plt.subplots(self.num_img, 2, figsize=(12, 3 * self.num_img))\n",
    "\n",
    "        for i, img_batch in enumerate(self.test_dataset.take(self.num_img)):\n",
    "            input_img = img_batch[0]  # Remove batch dim\n",
    "            translated_img = self.model.gen_G(tf.expand_dims(input_img, 0), training=False)[0]\n",
    "\n",
    "            # Convert from [-1, 1] → [0, 255]\n",
    "            input_img_np = ((input_img + 1.0) * 127.5).numpy().astype(np.uint8)\n",
    "            translated_img_np = ((translated_img + 1.0) * 127.5).numpy().astype(np.uint8)\n",
    "\n",
    "            ax[i, 0].imshow(input_img_np)\n",
    "            ax[i, 1].imshow(translated_img_np)\n",
    "            ax[i, 0].set_title(\"Input Photo\")\n",
    "            ax[i, 1].set_title(\"Translated Monet\")\n",
    "            ax[i, 0].axis(\"off\")\n",
    "            ax[i, 1].axis(\"off\")\n",
    "\n",
    "            # Save translated image\n",
    "            img_to_save = keras.utils.array_to_img(translated_img_np)\n",
    "            img_to_save.save(f\"{self.output_dir}/epoch_{epoch + 1}_img_{i}.png\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38509725",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_G = get_resnet_generator(name=\"generator_G\")  # Photo → Monet\n",
    "gen_F = get_resnet_generator(name=\"generator_F\")  # Monet → Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537db953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonetGAN(keras.Model):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super().__init__()\n",
    "        self.generator = generator  # Photo → Monet\n",
    "        self.discriminator = discriminator  # Real vs. Fake Monet\n",
    "\n",
    "    def compile(self, gen_optimizer, disc_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.gen_optimizer = gen_optimizer\n",
    "        self.disc_optimizer = disc_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.gen_loss_metric = keras.metrics.Mean(name=\"gen_loss\")\n",
    "        self.disc_loss_metric = keras.metrics.Mean(name=\"disc_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_metric, self.disc_loss_metric]\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        real_photos, real_monets = batch_data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Generate Monet-style images from real photos\n",
    "            fake_monets = self.generator(real_photos, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            disc_real_output = self.discriminator(real_monets, training=True)\n",
    "            disc_fake_output = self.discriminator(fake_monets, training=True)\n",
    "\n",
    "            # === Losses ===\n",
    "            # Generator tries to make fake_monets look real\n",
    "            gen_loss = self.loss_fn(tf.ones_like(disc_fake_output), disc_fake_output)\n",
    "\n",
    "            # Discriminator learns to distinguish real from fake\n",
    "            disc_real_loss = self.loss_fn(tf.ones_like(disc_real_output), disc_real_output)\n",
    "            disc_fake_loss = self.loss_fn(tf.zeros_like(disc_fake_output), disc_fake_output)\n",
    "            disc_loss = 0.5 * (disc_real_loss + disc_fake_loss)\n",
    "\n",
    "        # === Apply Gradients ===\n",
    "        gen_grads = tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        disc_grads = tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.gen_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "        self.disc_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # === Update Metrics ===\n",
    "        self.gen_loss_metric.update_state(gen_loss)\n",
    "        self.disc_loss_metric.update_state(disc_loss)\n",
    "\n",
    "        return {\n",
    "            \"gen_loss\": self.gen_loss_metric.result(),\n",
    "            \"disc_loss\": self.disc_loss_metric.result()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea0f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_gan = MonetGAN(generator=gen_G, discriminator=disc_Y)\n",
    "\n",
    "monet_gan.compile(\n",
    "    gen_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    loss_fn=keras.losses.MeanSquaredError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_gan.fit(\n",
    "    tf.data.Dataset.zip((train_photos, train_monets)),\n",
    "    epochs=50,\n",
    "    callbacks=[GANMonitor(test_dataset=test_photos)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c447d",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed46ed6",
   "metadata": {},
   "source": [
    "This model is designed to learn the style of Monet paintings and apply it to the real world images provided. The problem is that this model requires a lot of CPU and RAM to run and therefore will take over hours to generate the photos needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da625d43",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0faa1a9",
   "metadata": {},
   "source": [
    "Since my laptop is old and does not have a lot of CPU and RAM, I was not able to fully run this project, but I did write the code for a one-way GAN. Because of this, I have learned about the importance of considering efficiency and time complexity in the model, especially when hardware is hindering. \n",
    "\n",
    "If I had more time to work on this project, I would try to simplify the model so that I can run it. I would also try to tune hyperparameters and see what combination gives me the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1700eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281bbd19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107ba51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad849f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1100f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2c324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
